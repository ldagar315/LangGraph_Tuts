# LangGraph News Summarizer and Quiz Creator Agent

This repository contains a Python agent built using `LangGraph` that automates the process of finding recent news on a specific topic, summarizing it, and generating a multiple-choice quiz based on the summary.

The primary goal of this specific repository is to serve as a **comparison of the output generated by different large language models (LLMs) when given the same initial prompt/task in a "one-shot" manner** (i.e., building the agent based on the prompt). It also documents the bugs and challenges encountered while trying to implement and run the agent based on the model's generated code outline.

## Overview

The agent takes a user's topic of interest, a time frame (number of days), and an optional region as input. It then performs the following steps orchestrated by `LangGraph`:

1.  **Formulates Search Query:** Creates a query for a search tool based on user inputs.
2.  **Searches News:** Uses the Tavily Search API to find relevant news articles.
3.  **Processes Articles:** Selects the top articles (currently uses snippets provided by Tavily for simplicity).
4.  **Summarizes News:** Uses a Large Language Model (LLM) via Groq (specifically `llama3-70b-8192`) to generate a concise summary of the selected articles.
5.  **Checks Context:** (Optional) Asks the LLM if the generated summary provides sufficient context.
6.  **Expands Search (if needed):** If context is deemed insufficient and within attempt limits, formulates a new, more specific query and searches again.
7.  **Generates Quiz:** Uses the LLM to create a multiple-choice quiz (3 questions by default) based on the final summary content, ensuring questions test core understanding.
8.  **Outputs:** Returns the generated summary and the list of quiz questions.

![image](https://github.com/user-attachments/assets/ce451839-0956-4d9a-ad58-1747bcae1811)

<img width="1680" alt="Screenshot 2025-04-04 at 9 46 06 PM" src="https://github.com/user-attachments/assets/317a1928-84d1-4d1d-a7f3-64b7a8a47d14" />

<img width="1680" alt="Screenshot 2025-04-04 at 9 46 19 PM" src="https://github.com/user-attachments/assets/d1c65dcb-6b80-43ad-833f-8344a47749fd" />


## Features

* **Automated News Fetching:** Leverages `Tavily Search` for real-time news discovery.
* **LLM-Powered Summarization:** Utilizes `Groq`'s fast LLM inference for creating easy-to-understand summaries.
* **Context-Aware Quiz Generation:** Creates relevant MCQs using the `LLM` based on the summarized content.
* **State Management:** Uses `LangGraph`'s `StateGraph` to manage the flow and intermediate results.
* **Conditional Logic:** Implements conditional paths for context expansion based on LLM analysis.
* **Structured Output:** Uses `Pydantic` models and `LangChain`'s output parsers for reliable quiz generation format.

## How it Works

The agent is implemented as a state machine using `LangGraph`:

* **State (`AgentState`):** A `TypedDict` that holds all information, including inputs, intermediate results (queries, articles, summary), control flags (like `needs_expansion`), and final outputs (`summary`, `quiz`).
* **Nodes:** Python functions representing distinct processing steps (e.g., `search_news`, `summarize_news`, `generate_quiz`). Each node takes the current state, performs its action, and returns updates to the state.
* **Edges:** Define the flow between nodes. Conditional edges (`add_conditional_edges`) are used to direct the flow based on the current state (e.g., deciding whether to expand the search or generate the quiz).

## Technology Stack

* **Orchestration:** `LangGraph`
* **LLM Interactions:** `LangChain`, `Langchain-Groq`
* **LLM Provider:** `Groq` (using `Llama 3 70b`)
* **Search:** `Tavily Search API`
* **Data Validation:** `Pydantic` (v1 for LangChain compatibility)
* **Language:** `Python 3.9+`

## Setup and Running

1.  **Prerequisites:**
    * Python 3.9 or higher.
    * Access to `Groq API` and `Tavily Search API`.

2.  **Clone the Repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

3.  **Install Dependencies:**
    Create a `requirements.txt` file if one doesn't exist based on the imports in the script, or install manually:
    ```bash
    pip install -r requirements.txt
    ```
    ```bash
    # or install manually:
    pip install langgraph langchain langchain-groq langchain-community tavily-python "pydantic<2" python-dotenv
    ```
    *(Note: Pydantic v1 is often needed for compatibility with older LangChain versions; adjust if using newer versions)*

4.  **Set API Keys:**
    This agent requires API keys for `Groq` and `Tavily`. It is crucial to set these as environment variables.

    * **Linux/macOS:**
        ```bash
        export GROQ_API_KEY="your_groq_api_key_here"
        export TAVILY_API_KEY="your_tavily_api_key_here"
        ```
    * **Windows (Command Prompt):**
        ```bash
        set GROQ_API_KEY=your_groq_api_key_here
        set TAVILY_API_KEY=your_tavily_api_key_here
        ```
    * **Windows (PowerShell):**
        ```powershell
        $env:GROQ_API_KEY = "your_groq_api_key_here"
        $env:TAVILY_API_KEY = "your_tavily_api_key_here"
        ```

    Alternatively, you can use a `.env` file and load it using `python-dotenv` if you modify the script accordingly (though environment variables are generally preferred for security).

5.  **Run the Agent:**
    Execute the Python script:
    ```bash
    python your_agent_script_name.py
    ```
    The script will run with default inputs defined in the `if __name__ == "__main__":` block. You can modify these inputs directly in the script to test different topics, regions, or time frames.

### Input

The agent expects the following inputs (currently hardcoded in the main block, but could be adapted for dynamic input):

* `input_topic` (`str`): The news topic you are interested in (e.g., "AI regulation").
* `input_num_days` (`int`): How many past days to search within (e.g., 7).
* `input_region` (`str`): The geographical focus (e.g., "USA", "Europe", "Global").

### Output

The script will print the execution steps to the console. The final state will contain:

* `summary` (`str`): The generated news summary.
* `quiz` (`List[QuizQuestion]`): A list of `Pydantic` `QuizQuestion` objects, each containing the question text, four options, and the correct option.

The final summary and quiz are printed at the end of the execution.

### Configuration

Key parameters can be adjusted at the top of the script:

* `LLM_MODEL_NAME`: Change the Groq model if needed.
* `MAX_ARTICLES_TO_PROCESS`: How many articles to use for summarization/quiz generation.
* `NUM_QUIZ_QUESTIONS`: The desired number of quiz questions.
* `MAX_EXPANSION_ATTEMPTS`: How many times the agent can try to expand the search for better context.

### Potential Issues & Improvements

* **Content Fetching:** The current version relies on snippets from `Tavily`. A robust implementation would involve fetching and parsing the full content of articles, requiring additional libraries (like `BeautifulSoup`, `requests`) and more complex error handling.
* **Error Handling:** Basic error handling is included, but real-world scenarios might require more sophisticated handling of API errors, timeouts, malformed content, etc.
* **LLM Reliability:** `LLM` outputs (summary, quiz, analysis) can vary. Structured output parsing helps, but validation and potential retry logic could improve robustness.
* **Context Window:** Very long articles might exceed the `LLM`'s context window. Strategies like map-reduce summarization could be necessary.

### Model Comparison Note

As stated initially, this repository primarily serves as an example derived from an `LLM` prompt. The focus is on comparing how well different models interpret the requirements and generate a working code structure, and documenting the necessary debugging steps to make the generated code functional.
